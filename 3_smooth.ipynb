{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据稀疏和数据平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from match import Match\n",
    "from hmm import HMM\n",
    "from nlputils import * \n",
    "from bigram import Bigram\n",
    "\n",
    "import jieba\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "text_file_path = './corpus/2021.txt'\n",
    "dest_folder = './states/'\n",
    "states_file_path = dest_folder + os.path.basename(text_file_path)\n",
    "# text_to_state(text_file_path, dest_folder)\n",
    "corpus = read_corpus_or_states_for_hmm(text_file_path)\n",
    "states = read_corpus_or_states_for_hmm(states_file_path)\n",
    "\n",
    "train, test = yield_data(zip(corpus, states), shuffle=False)\n",
    "corpus_train, states_train = zip(*train)\n",
    "corpus_test, states_test = zip(*test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练二元文法模型和HMM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = Bigram()\n",
    "bigram_model.train(corpus=corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM()\n",
    "hmm.train(corpus_train, states_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用句子测试数据稀疏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6496403599350137e-16 [{('<BOS>', '我'): 492, '<BOS>': 340964}, {('我', '不'): 75, '我': 18548}, {('不', '知道'): 213, '不': 20832}, {('知道', '你'): 18, '知道': 1039}, {('你', '有'): 19, '你': 2273}, {('有', '多少'): 105, '有': 34660}, {('多少', '钱'): 46, '多少': 795}, {('钱', '<EOS>'): 1, '钱': 918}]\n",
      "0.0 [{('<BOS>', '中国'): 3259, '<BOS>': 340964}, {('中国', '国家'): 194, '中国': 76064}, {('国家', '航天'): 28, '国家': 39369}, {('航天', '总局'): 0, '航天': 1272}, {('总局', '发表'): 0, '总局': 344}, {('发表', '演讲'): 22, '发表': 2063}, {('演讲', '<EOS>'): 0, '演讲': 367}]\n",
      "0.0 [{('<BOS>', '美国'): 362, '<BOS>': 340964}, {('美国', '在'): 147, '美国': 6267}, {('在', '下个'): 2, '在': 137717}, {('下个', '月'): 0, '下个': 10}, {('月', '有'): 35, '月': 43971}, {('有', '十条'): 0, '有': 34660}, {('十条', '裤子'): 0, '十条': 50}, {('裤子', '穿'): 0, '裤子': 7}, {('穿', '<EOS>'): 0, '穿': 550}]\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "    \"我不知道你有多少钱\",\n",
    "    \"中国国家航天总局发表演讲\",\n",
    "    \"美国在下个月有十条裤子穿\"\n",
    "]\n",
    "\n",
    "for t in text:\n",
    "    states, res = hmm.tokenize(t)\n",
    "    prob, details = bigram_model.get_prob(res)\n",
    "    print(prob, details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用bigram模块中基于加一法实现的数据平滑，再测试上述句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.022257637733543e+120\n",
      "1.830890240596848e+121\n",
      "5.287413711904295e+108\n"
     ]
    }
   ],
   "source": [
    "for t in text:\n",
    "    states, res = hmm.tokenize(t)\n",
    "    prob, details = bigram_model.get_prob_smooth(res)\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练正向逆向分词模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = corpus_to_vocab(corpus_train)\n",
    "vocab_train[:10] \n",
    "forward_match = Match(\"max_forward\", vocab_train)\n",
    "backwad_match = Match(\"max_backward\", vocab_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用数据平滑后的消岐函数进行消岐，并计算PRF值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "jieba_res = []\n",
    "output_folder = './results/'\n",
    "f = open(os.path.join(output_folder, 'disambiguation_result_smooth.txt'), 'w', encoding='utf-8')\n",
    "for test in corpus_test:\n",
    "    # test是jieba分词的结果\n",
    "    test = test.replace(\" \", \"\")\n",
    "    states, hmm_tokens = hmm.tokenize(test)\n",
    "    forward_tokens = forward_match.tokenize(test)\n",
    "    backward_tokens = backwad_match.tokenize(test)\n",
    "    jieba_tokens = jieba.lcut(test, cut_all=False)\n",
    "\n",
    "    tokens = bigram_model.disambiguation([hmm_tokens, forward_tokens, backward_tokens], smooth=True)\n",
    "    \n",
    "    # 写入\n",
    "    for token in tokens:\n",
    "        if token != tokens[-1]:\n",
    "            f.write(token + ' ')\n",
    "        else:\n",
    "            f.write(token + '\\n')\n",
    "            \n",
    "    res += tokens\n",
    "    jieba_res += jieba_tokens\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9043959898725205, 0.8980334262176466, 0.9012034781417373)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real, pred, correct = cal_count(jieba_res, res)\n",
    "p, r, f = cal_prf(real, pred, correct)\n",
    "p, r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试在未平滑下出现数据稀疏导致随机选择的概率的的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9022195631261585"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need = 0\n",
    "hmms = []\n",
    "forward = []\n",
    "backwad = []\n",
    "for text in corpus_test:\n",
    "    text = text.replace(\" \", \"\")\n",
    "    states, hmm_tokens = hmm.tokenize(text)\n",
    "    forward_tokens = forward_match.tokenize(text)\n",
    "    backward_tokens = backwad_match.tokenize(text)\n",
    "\n",
    "    hmms.append(hmm_tokens)\n",
    "    forward.append(forward_tokens)\n",
    "    backwad.append(backward_tokens)\n",
    "\n",
    "    if bigram_model.get_prob(hmm_tokens)[0] == 0 and bigram_model.get_prob(forward_tokens)[0] == 0 and bigram_model.get_prob(backward_tokens)[0] == 0:\n",
    "        need += 1\n",
    "need/len(corpus_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
